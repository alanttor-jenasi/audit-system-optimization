"""
QAæŸ¥é‡æ¨¡å— - ä½¿ç”¨BGEåµŒå…¥æ¨¡å‹ + ä½™å¼¦ç›¸ä¼¼åº¦
====================================

åŠŸèƒ½:
1. åŠ è½½å·²å®¡æ ¸çŸ¥è¯†åº“çš„æ‰€æœ‰QA
2. ä½¿ç”¨BGEæ¨¡å‹ç”Ÿæˆå‘é‡
3. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
4. è¿”å›é‡å¤ç»„
"""

import requests
import numpy as np
from typing import List, Dict, Tuple
import logging
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'mcp_services'))
from common.config import BASE_CONFIG

logger = logging.getLogger(__name__)


class DuplicateChecker:
    """QAæŸ¥é‡å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æŸ¥é‡å™¨"""
        # BGEåµŒå…¥æ¨¡å‹é…ç½®
        self.embedding_url = BASE_CONFIG['embedding']['url']
        self.embedding_model = BASE_CONFIG['embedding']['model']
        self.embedding_timeout = BASE_CONFIG['embedding']['timeout']
        
        logger.info(f"âœ… æŸ¥é‡å™¨åˆå§‹åŒ–å®Œæˆ [æ¨¡å‹={self.embedding_model}, æœåŠ¡={self.embedding_url}]")
    
    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        è°ƒç”¨BGEæ¨¡å‹ç”Ÿæˆæ–‡æœ¬å‘é‡ (OpenAIå…¼å®¹æ¥å£)
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡çŸ©é˜µ (n, 1024)
        """
        try:
            # ä½¿ç”¨OpenAIå…¼å®¹çš„APIæ¥å£
            response = requests.post(
                f"{self.embedding_url}/v1/embeddings",
                json={
                    "input": texts,
                    "model": self.embedding_model
                },
                timeout=self.embedding_timeout
            )
            response.raise_for_status()
            result = response.json()
            
            # è§£æOpenAIæ ¼å¼çš„å“åº”
            embeddings = [item['embedding'] for item in result['data']]
            embeddings = np.array(embeddings)
            logger.info(f"âœ… å‘é‡ç”ŸæˆæˆåŠŸ [æ•°é‡={len(texts)}, ç»´åº¦={embeddings.shape}]")
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2
            
        Returns:
            ç›¸ä¼¼åº¦ (0-1)
        """
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def find_duplicates(
        self, 
        segments: List[Dict], 
        similarity_threshold: float = 0.85,
        batch_size: int = 100
    ) -> List[List[Dict]]:
        """
        æŸ¥æ‰¾é‡å¤çš„QAåˆ†æ®µ
        
        Args:
            segments: åˆ†æ®µåˆ—è¡¨,æ¯ä¸ªåŒ…å« {id, question, answer, document_id, document_name}
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            é‡å¤ç»„åˆ—è¡¨,æ¯ç»„åŒ…å«ç›¸ä¼¼çš„åˆ†æ®µ
        """
        if not segments:
            return []
        
        logger.info(f"ğŸ” å¼€å§‹æŸ¥é‡ [æ€»æ•°={len(segments)}, é˜ˆå€¼={similarity_threshold}]")
        
        # 1. å‡†å¤‡æ–‡æœ¬æ•°æ®(é—®é¢˜+ç­”æ¡ˆ)
        texts = []
        for seg in segments:
            # ç»„åˆé—®é¢˜å’Œç­”æ¡ˆä½œä¸ºå®Œæ•´æ–‡æœ¬
            combined_text = f"é—®:{seg['question']}\nç­”:{seg['answer']}"
            texts.append(combined_text)
        
        # 2. åˆ†æ‰¹ç”Ÿæˆå‘é‡(é¿å…å†…å­˜æº¢å‡º)
        all_embeddings = []
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_embeddings = self.get_embeddings(batch_texts)
            all_embeddings.append(batch_embeddings)
            logger.info(f"ğŸ“Š è¿›åº¦: {min(i+batch_size, len(texts))}/{len(texts)}")
        
        # åˆå¹¶æ‰€æœ‰å‘é‡
        embeddings = np.vstack(all_embeddings)
        
        # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        logger.info("ğŸ§® è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
        n = len(segments)
        visited = set()
        duplicate_groups = []
        
        for i in range(n):
            if i in visited:
                continue
            
            # å½“å‰åˆ†æ®µçš„é‡å¤ç»„
            current_group = [(segments[i], 1.0)]  # (segment, similarity_to_first)
            visited.add(i)
            
            # ä¸åç»­åˆ†æ®µæ¯”è¾ƒ
            for j in range(i+1, n):
                if j in visited:
                    continue
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = self.cosine_similarity(embeddings[i], embeddings[j])
                
                if similarity >= similarity_threshold:
                    current_group.append((segments[j], float(similarity)))
                    visited.add(j)
            
            # åªä¿ç•™æœ‰é‡å¤çš„ç»„(è‡³å°‘2ä¸ª)
            if len(current_group) >= 2:
                # æ·»åŠ ç›¸ä¼¼åº¦ä¿¡æ¯åˆ°æ¯ä¸ªåˆ†æ®µ
                for seg, sim in current_group:
                    seg['similarity_score'] = sim
                # åªä¿å­˜segmentå¯¹è±¡
                duplicate_groups.append([seg for seg, _ in current_group])
        
        logger.info(f"âœ… æŸ¥é‡å®Œæˆ [å‘ç°{len(duplicate_groups)}ä¸ªé‡å¤ç»„]")
        return duplicate_groups
    
    def format_duplicate_groups(self, duplicate_groups: List[List[Dict]]) -> Dict:
        """
        æ ¼å¼åŒ–é‡å¤ç»„ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        
        Args:
            duplicate_groups: é‡å¤ç»„åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–åçš„æ•°æ®
        """
        formatted_groups = []
        
        for idx, group in enumerate(duplicate_groups, 1):
            # è®¡ç®—ç»„å†…å¹³å‡ç›¸ä¼¼åº¦(é™¤äº†ç¬¬ä¸€ä¸ª100%)
            similarities = [seg.get('similarity_score', 0.0) for seg in group]
            avg_similarity = sum(similarities) / len(similarities) if similarities else 0.0
            
            # ç»„å†…åˆ†æ®µæŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
            sorted_group = sorted(group, key=lambda x: x.get('similarity_score', 0.0), reverse=True)
            
            formatted_group = {
                'group_id': idx,
                'similarity': round(avg_similarity * 100, 1),  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                'count': len(group),
                'items': [
                    {
                        'segment_id': seg['id'],
                        'document_id': seg['document_id'],
                        'document_name': seg['document_name'],
                        'classification': seg.get('classification', '-'),
                        'question': seg['question'],
                        'answer': seg['answer'],
                        'similarity': round(seg.get('similarity_score', 0.0) * 100, 1),
                        'created_at': seg.get('created_at', 0),
                        'updated_at': seg.get('updated_at', 0)
                    }
                    for seg in sorted_group
                ]
            }
            formatted_groups.append(formatted_group)
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        formatted_groups.sort(key=lambda x: x['similarity'], reverse=True)
        
        return {
            'total_groups': len(formatted_groups),
            'total_duplicates': sum(g['count'] for g in formatted_groups),
            'groups': formatted_groups
        }
