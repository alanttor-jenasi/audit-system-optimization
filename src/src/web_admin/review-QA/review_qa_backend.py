"""
QAå®¡æ ¸ä¸ä¿®æ­£ç³»ç»Ÿ - åç«¯æœåŠ¡
====================================

åŠŸèƒ½ï¼š
1. æä¾›DifyçŸ¥è¯†åº“åˆ†æ®µçš„å¢åˆ æ”¹æŸ¥æ¥å£
2. æ”¯æŒæœªå®¡æ ¸åŒºåŸŸå’Œå·²å®¡æ ¸åŒºåŸŸçš„æ•°æ®ç®¡ç†
3. å¤„ç†QAçš„å®¡æ ¸ã€ç¼–è¾‘ã€åˆ†ç±»å’Œè½¬ç§»
"""

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import requests
import sys
import re
import time
from pathlib import Path
from datetime import datetime, timezone, timedelta
import logging
from duplicate_checker import DuplicateChecker

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'mcp_services'))
from common.config import BASE_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

app = Flask(__name__, 
            template_folder='.',
            static_folder='static')
CORS(app)

# Difyé…ç½®
DIFY_CONFIG = BASE_CONFIG['dify']
DIFY_API_KEY = DIFY_CONFIG['api_key']
DIFY_BASE_URL = DIFY_CONFIG['api_base']

# çŸ¥è¯†åº“é…ç½®
UNREVIEWED_DATASET_ID = "1397b9d1-8e25-4269-ba12-046059a425b6"  # æœªå®¡æ ¸çŸ¥è¯†åº“
REVIEWED_DATASET_ID = "2df8ca5b-ac31-4dba-8b48-fc09f678b62d"    # å·²å®¡æ ¸çŸ¥è¯†åº“

# æœ¬åœ°APIé…ç½®
LOCAL_QUERY_API_BASE = "http://192.168.1.138:49154/api/local/query"

# æœªå®¡æ ¸çŸ¥è¯†åº“çš„æ–‡æ¡£é…ç½®
UNREVIEWED_DOCUMENTS = {
    "1a92b558-2051-4ebc-9441-2209dfd356b8": "æ—§QA",
    "ee3a5cb0-3fa9-4cd1-9a1a-113bc43b5d5a": "å¾®ä¿¡æ¯æ—¥QA",
    "a025564c-33b4-458e-835b-324ac75c0e24": "äººå·¥/ç”¨æˆ·æ·»åŠ "
}

# å·²å®¡æ ¸çŸ¥è¯†åº“çš„æ–‡æ¡£é…ç½®
REVIEWED_DOCUMENTS = {
    "e4d103ba-ab38-4c0b-8c4d-5fd65da451e0": "æ¥çº¿ç±»",
    "6ed1a963-f4f4-4755-8f58-65ed4ccad67e": "ç”µæœºç±»",
    "0f615db6-35be-40b8-ad48-34db22ed2fb0": "è§¦æ‘¸å±ç±»",
    "b22e210a-0bc8-496a-9828-c6016389bca2": "ç¨‹åºç±»",
    "d894cff9-c9aa-4d56-a8ae-d09f979779bf": "äº§å“å‹å·åŠŸèƒ½ç±»",
    "fce7c466-da39-4c37-a281-225087f29dee": "äº§å“ç»´ä¿®ç±»",
    "4bc158d8-72e1-4881-a3c9-75d94f0c9e2a": "äº§å“åŠŸèƒ½ç±»",
    "55e92a15-cc40-49de-a69d-2ef9e863a88a": "modbusé€šä¿¡åœ°å€è¡¨_SENç±»",
    "8f4f53d9-8a48-4a0a-aad3-b14b96a46c93": "äº§å“çŸ¥è¯†ç±»",
    "9ac2c969-aea2-40a5-a57d-91b98e9421a2": "é€šä¿¡å‚æ•°ç±»",
    "56f3277a-46d5-4dc0-9d1b-c86b92b979cd": "ä¸‹è½½åŠŸèƒ½ç±»",
    "dbb66ae8-4d9a-4ea9-b5de-603f8d18e1b6": "å’¨è¯¢ç±»",
    "175f56a9-47ec-4c8f-b75e-cd57d8c99627": "é€šè®¯ç±»",
    "010a4033-033e-456d-8e13-452d86cb2c16": "æ“ä½œç±»"
}


class DifyAPIClient:
    """Dify APIå®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.base_url = DIFY_BASE_URL
        self.api_key = DIFY_API_KEY
        self.headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
    
    def get_segment(self, dataset_id: str, document_id: str, segment_id: str):
        """è·å–å•ä¸ªåˆ†æ®µï¼ˆæœ€ä¼˜æ–¹æ¡ˆï¼‰"""
        url = f"{self.base_url}/datasets/{dataset_id}/documents/{document_id}/segments/{segment_id}"
        
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            result = response.json()
            return {'success': True, 'data': result.get('data')}
        except Exception as e:
            logger.error(f"è·å–åˆ†æ®µå¤±è´¥ [segment_id={segment_id}]: {e}")
            return {'success': False, 'error': str(e)}
    
    def get_document_segments(self, dataset_id: str, document_id: str, page: int = 1, limit: int = 100):
        """è·å–æ–‡æ¡£çš„æ‰€æœ‰åˆ†æ®µ"""
        url = f"{self.base_url}/datasets/{dataset_id}/documents/{document_id}/segments"
        params = {'page': page, 'limit': limit}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            return {'success': True, 'data': response.json()}
        except Exception as e:
            logger.error(f"è·å–åˆ†æ®µå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def get_all_segments(self, dataset_id: str, document_id: str):
        """è·å–æ–‡æ¡£çš„æ‰€æœ‰åˆ†æ®µï¼ˆå¤„ç†åˆ†é¡µï¼‰"""
        all_segments = []
        page = 1
        
        while True:
            result = self.get_document_segments(dataset_id, document_id, page=page, limit=100)
            if not result['success']:
                return result
            
            data = result['data']
            segments = data.get('data', [])
            all_segments.extend(segments)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
            if not data.get('has_more', False):
                break
            
            page += 1
        
        return {'success': True, 'data': all_segments}
    
    def update_segment(self, dataset_id: str, document_id: str, segment_id: str, content: str, keywords: list = None):
        """æ›´æ–°åˆ†æ®µå†…å®¹"""
        url = f"{self.base_url}/datasets/{dataset_id}/documents/{document_id}/segments/{segment_id}"
        payload = {'segment': {'content': content}}
        
        if keywords:
            payload['segment']['keywords'] = keywords
        
        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=30)
            response.raise_for_status()
            logger.info(f"âœ… åˆ†æ®µæ›´æ–°æˆåŠŸ [segment_id={segment_id}]")
            return {'success': True, 'data': response.json()}
        except Exception as e:
            logger.error(f"âŒ åˆ†æ®µæ›´æ–°å¤±è´¥ [segment_id={segment_id}]: {e}")
            return {'success': False, 'error': str(e)}
    
    def delete_segment(self, dataset_id: str, document_id: str, segment_id: str):
        """åˆ é™¤åˆ†æ®µ"""
        url = f"{self.base_url}/datasets/{dataset_id}/documents/{document_id}/segments/{segment_id}"
        
        try:
            response = requests.delete(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            logger.info(f"âœ… åˆ†æ®µåˆ é™¤æˆåŠŸ [segment_id={segment_id}]")
            return {'success': True}
        except Exception as e:
            logger.error(f"âŒ åˆ†æ®µåˆ é™¤å¤±è´¥ [segment_id={segment_id}]: {e}")
            return {'success': False, 'error': str(e)}
    
    def add_segment(self, dataset_id: str, document_id: str, content: str, keywords: list = None):
        """æ·»åŠ åˆ†æ®µ"""
        url = f"{self.base_url}/datasets/{dataset_id}/documents/{document_id}/segments"
        payload = {
            'segments': [{
                'content': content,
                'keywords': keywords or []
            }]
        }
        
        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=30)
            response.raise_for_status()
            logger.info(f"âœ… åˆ†æ®µæ·»åŠ æˆåŠŸ [document_id={document_id}]")
            return {'success': True, 'data': response.json()}
        except Exception as e:
            logger.error(f"âŒ åˆ†æ®µæ·»åŠ å¤±è´¥ [document_id={document_id}]: {e}")
            return {'success': False, 'error': str(e)}


def parse_qa_content(content: str):
    """ä»åˆ†æ®µå†…å®¹ä¸­è§£æé—®ç­”å¯¹å’Œå…ƒæ•°æ®"""
    lines = content.split('\n')
    question = ""
    answer = ""
    source = ""
    add_type = ""
    classification = ""  # æ–°å¢åˆ†ç±»å­—æ®µ
    
    # çŠ¶æ€æ ‡è®°ï¼šå½“å‰æ­£åœ¨æ”¶é›†å“ªä¸ªå­—æ®µ
    collecting = None
    
    for line in lines:
        line_stripped = line.strip()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„å­—æ®µå¼€å§‹
        if line_stripped.startswith('é—®:') or line_stripped.startswith('é—®ï¼š'):
            question = line_stripped[2:].strip()
            collecting = 'question' if not question else None
        elif line_stripped.startswith('ç­”:') or line_stripped.startswith('ç­”ï¼š'):
            answer = line_stripped[2:].strip()
            collecting = 'answer'  # å¼€å§‹æ”¶é›†ç­”æ¡ˆï¼ˆå³ä½¿å½“å‰è¡Œä¸ºç©ºï¼‰
        elif line_stripped.startswith('#source#:') or line_stripped.startswith('#source#ï¼š'):
            # æ”¯æŒä¸­æ–‡å†’å·å’Œè‹±æ–‡å†’å·
            if ':' in line_stripped:
                source = line_stripped.split(':', 1)[1].strip()
            elif 'ï¼š' in line_stripped:
                source = line_stripped.split('ï¼š', 1)[1].strip()
            collecting = 'source' if not source else None
        elif line_stripped.startswith('classification:') or line_stripped.startswith('classificationï¼š'):
            # è§£æåˆ†ç±»å­—æ®µ
            if ':' in line_stripped:
                classification = line_stripped.split(':', 1)[1].strip()
            elif 'ï¼š' in line_stripped:
                classification = line_stripped.split('ï¼š', 1)[1].strip()
            collecting = None
        elif line_stripped.startswith('æ·»åŠ äººå‘˜:') or line_stripped.startswith('æ·»åŠ äººå‘˜ï¼š'):
            # æ”¯æŒä¸­æ–‡å†’å·å’Œè‹±æ–‡å†’å·
            if ':' in line_stripped:
                add_type = line_stripped.split(':', 1)[1].strip()
            elif 'ï¼š' in line_stripped:
                add_type = line_stripped.split('ï¼š', 1)[1].strip()
            collecting = None
        elif line_stripped and collecting:
            # ç»§ç»­æ”¶é›†å½“å‰å­—æ®µçš„å†…å®¹
            if collecting == 'question':
                question += ('\n' if question else '') + line_stripped
            elif collecting == 'answer':
                answer += ('\n' if answer else '') + line_stripped
            elif collecting == 'source':
                source += ('\n' if source else '') + line_stripped
    
    return {
        'question': question.strip(),
        'answer': answer.strip(),
        'source': source.strip(),
        'add_type': add_type.strip(),
        'classification': classification.strip()  # è¿”å›åˆ†ç±»å­—æ®µ
    }


def clean_qa_content(content: str) -> str:
    """
    æ¸…ç†å’Œè§„èŒƒåŒ–QAå†…å®¹
    
    æ¸…ç†è§„åˆ™ï¼ˆå‚è€ƒparse_qa_contentçš„é€»è¾‘ï¼Œä½†ä¸è°ƒç”¨å®ƒï¼‰ï¼š
    1. è§£æcontentæå–å„ä¸ªå­—æ®µï¼ˆé—®ã€ç­”ã€sourceã€æ·»åŠ äººå‘˜ã€åˆ†ç±»ï¼‰
    2. æ¸…ç†å„ä¸ªå­—æ®µï¼šå»é™¤å¤šä½™ç©ºæ ¼ã€ç©ºè¡Œ
    3. ç»Ÿä¸€æ ¼å¼ï¼šç»Ÿä¸€ä½¿ç”¨ä¸­æ–‡å†’å·ï¼ˆé—®ï¼šã€ç­”ï¼šï¼‰
    4. é‡æ–°æ ¼å¼åŒ–ä¸ºæ ‡å‡†æ ¼å¼
    
    Args:
        content: åŸå§‹contentå­—ç¬¦ä¸²
        
    Returns:
        æ¸…ç†åçš„contentå­—ç¬¦ä¸²
    """
    if not content:
        return content
    
    lines = content.split('\n')
    question = ""
    answer = ""
    source = ""
    add_type = ""
    classification = ""
    
    # çŠ¶æ€æ ‡è®°ï¼šå½“å‰æ­£åœ¨æ”¶é›†å“ªä¸ªå­—æ®µ
    collecting = None
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾"ç­”:"æˆ–"ç­”ï¼š"æ¥åˆ†å‰²é—®é¢˜å’Œç­”æ¡ˆï¼ˆå¤„ç†é—®å’Œç­”åœ¨åŒä¸€è¡Œçš„æƒ…å†µï¼‰
    answer_match = re.search(r'ç­”[ï¼š:]', content)
    
    if answer_match:
        # æ‰¾åˆ°"ç­”:"æˆ–"ç­”ï¼š"ï¼Œåˆ†å‰²é—®é¢˜å’Œç­”æ¡ˆ
        question_part = content[:answer_match.start()].strip()
        answer_part = content[answer_match.end():].strip()
        
        # å»é™¤é—®é¢˜éƒ¨åˆ†å¼€å¤´çš„"é—®:"æˆ–"é—®ï¼š"
        question_part = re.sub(r'^é—®[ï¼š:]\s*', '', question_part)
        question = question_part.strip()
        
        # å¤„ç†ç­”æ¡ˆéƒ¨åˆ†ï¼Œéœ€è¦æå–sourceã€classificationã€add_type
        answer_lines = answer_part.split('\n')
        answer_content = []
        
        for line in answer_lines:
            line_stripped = line.strip()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…ƒæ•°æ®æ ‡ç­¾ï¼ˆå¯èƒ½åœ¨è¡Œå†…ï¼‰
            if '#source#' in line_stripped or 'source#' in line_stripped:
                # æå–source
                source_match = re.search(r'#?source#?[ï¼š:]\s*(.+)', line_stripped)
                if source_match:
                    source = source_match.group(1).strip()
                # å¦‚æœsourceåœ¨è¡Œå†…ï¼Œåªå–å‰é¢çš„éƒ¨åˆ†ä½œä¸ºç­”æ¡ˆ
                if '#source#' in line_stripped or 'source#' in line_stripped:
                    before_source = re.split(r'#?source#?[ï¼š:]', line_stripped)[0].strip()
                    if before_source:
                        answer_content.append(before_source)
                break
            elif 'classification' in line_stripped.lower():
                # æå–classification
                class_match = re.search(r'classification[ï¼š:]\s*(.+)', line_stripped, re.IGNORECASE)
                if class_match:
                    classification = class_match.group(1).strip()
                # å¦‚æœclassificationåœ¨è¡Œå†…ï¼Œåªå–å‰é¢çš„éƒ¨åˆ†
                if 'classification' in line_stripped.lower():
                    before_class = re.split(r'classification[ï¼š:]', line_stripped, flags=re.IGNORECASE)[0].strip()
                    if before_class:
                        answer_content.append(before_class)
                break
            elif 'æ·»åŠ äººå‘˜' in line_stripped:
                # æå–add_type
                add_match = re.search(r'æ·»åŠ äººå‘˜[ï¼š:]\s*(.+)', line_stripped)
                if add_match:
                    add_type = add_match.group(1).strip()
                # å¦‚æœadd_typeåœ¨è¡Œå†…ï¼Œåªå–å‰é¢çš„éƒ¨åˆ†
                if 'æ·»åŠ äººå‘˜' in line_stripped:
                    before_add = line_stripped.split('æ·»åŠ äººå‘˜')[0].strip()
                    if before_add:
                        answer_content.append(before_add)
                break
            else:
                answer_content.append(line_stripped)
        
        answer = '\n'.join(answer_content).strip()
        
        # å¦‚æœè¿˜æ²¡æœ‰æå–åˆ°sourceã€classificationã€add_typeï¼Œç»§ç»­ä»å‰©ä½™å†…å®¹ä¸­æå–
        remaining_content = '\n'.join(answer_lines[len(answer_content):])
        for line in remaining_content.split('\n'):
            line_stripped = line.strip()
            if line_stripped.startswith('#source#:') or line_stripped.startswith('#source#ï¼š'):
                if ':' in line_stripped:
                    source = line_stripped.split(':', 1)[1].strip()
                elif 'ï¼š' in line_stripped:
                    source = line_stripped.split('ï¼š', 1)[1].strip()
            elif line_stripped.startswith('classification:') or line_stripped.startswith('classificationï¼š'):
                if ':' in line_stripped:
                    classification = line_stripped.split(':', 1)[1].strip()
                elif 'ï¼š' in line_stripped:
                    classification = line_stripped.split('ï¼š', 1)[1].strip()
            elif line_stripped.startswith('æ·»åŠ äººå‘˜:') or line_stripped.startswith('æ·»åŠ äººå‘˜ï¼š'):
                if ':' in line_stripped:
                    add_type = line_stripped.split(':', 1)[1].strip()
                elif 'ï¼š' in line_stripped:
                    add_type = line_stripped.split('ï¼š', 1)[1].strip()
    else:
        # å›é€€åˆ°åŸå§‹çš„è¡Œ-by-lineè§£æé€»è¾‘
        for line in lines:
            line_stripped = line.strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„å­—æ®µå¼€å§‹
            if line_stripped.startswith('é—®:') or line_stripped.startswith('é—®ï¼š'):
                question = line_stripped[2:].strip()
                collecting = 'question' if not question else None
            elif line_stripped.startswith('ç­”:') or line_stripped.startswith('ç­”ï¼š'):
                answer = line_stripped[2:].strip()
                collecting = 'answer'
            elif line_stripped.startswith('#source#:') or line_stripped.startswith('#source#ï¼š'):
                if ':' in line_stripped:
                    source = line_stripped.split(':', 1)[1].strip()
                elif 'ï¼š' in line_stripped:
                    source = line_stripped.split('ï¼š', 1)[1].strip()
                collecting = None
            elif line_stripped.startswith('classification:') or line_stripped.startswith('classificationï¼š'):
                if ':' in line_stripped:
                    classification = line_stripped.split(':', 1)[1].strip()
                elif 'ï¼š' in line_stripped:
                    classification = line_stripped.split('ï¼š', 1)[1].strip()
                collecting = None
            elif line_stripped.startswith('æ·»åŠ äººå‘˜:') or line_stripped.startswith('æ·»åŠ äººå‘˜ï¼š'):
                if ':' in line_stripped:
                    add_type = line_stripped.split(':', 1)[1].strip()
                elif 'ï¼š' in line_stripped:
                    add_type = line_stripped.split('ï¼š', 1)[1].strip()
                collecting = None
            elif line_stripped and collecting:
                # ç»§ç»­æ”¶é›†å½“å‰å­—æ®µçš„å†…å®¹
                if collecting == 'question':
                    question += ('\n' if question else '') + line_stripped
                elif collecting == 'answer':
                    answer += ('\n' if answer else '') + line_stripped
    
    # æ¸…ç†å„ä¸ªå­—æ®µ
    question = question.strip()
    answer = answer.strip()
    source = source.strip()
    add_type = add_type.strip()
    classification = classification.strip()
    
    # æ¸…ç†é—®é¢˜ï¼šå»é™¤å¤šä½™ç©ºæ ¼ï¼Œä½†ä¿ç•™æ¢è¡Œï¼ˆå¦‚æœæœ‰ï¼‰
    if question:
        # å»é™¤æ¯è¡Œé¦–å°¾ç©ºæ ¼ï¼Œå»é™¤ç©ºè¡Œ
        question_lines = question.split('\n')
        question = '\n'.join(line.strip() for line in question_lines if line.strip())
        # å°†å¤šä¸ªè¿ç»­ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼ï¼ˆä½†ä¿ç•™æ¢è¡Œï¼‰
        question = re.sub(r' +', ' ', question)
    
    # æ¸…ç†ç­”æ¡ˆï¼šå»é™¤å¤šä½™ç©ºè¡Œï¼Œä½†ä¿ç•™å¿…è¦çš„æ¢è¡Œ
    if answer:
        # å»é™¤è¿ç»­çš„ç©ºè¡Œï¼ˆè¶…è¿‡2ä¸ªæ¢è¡Œç¬¦çš„æ›¿æ¢ä¸º2ä¸ªï¼‰
        answer = re.sub(r'\n{3,}', '\n\n', answer)
        # å»é™¤æ¯è¡Œé¦–å°¾ç©ºæ ¼
        answer_lines = answer.split('\n')
        answer = '\n'.join(line.strip() for line in answer_lines if line.strip())
    
    # æ¸…ç†sourceã€add_typeã€classificationï¼šå»é™¤å¤šä½™ç©ºæ ¼
    source = ' '.join(source.split()) if source else ''
    add_type = ' '.join(add_type.split()) if add_type else ''
    classification = ' '.join(classification.split()) if classification else ''
    
    # é‡æ–°æ ¼å¼åŒ–ä¸ºæ ‡å‡†æ ¼å¼ï¼ˆç»Ÿä¸€ä½¿ç”¨ä¸­æ–‡å†’å·ï¼‰
    cleaned_parts = []
    
    if question:
        cleaned_parts.append(f"é—®ï¼š{question}")
    
    if answer:
        cleaned_parts.append(f"ç­”ï¼š{answer}")
    
    if source:
        cleaned_parts.append(f"#source#:{source}")
    
    if add_type:
        cleaned_parts.append(f"æ·»åŠ äººå‘˜:{add_type}")
    
    if classification:
        cleaned_parts.append(f"åˆ†ç±»:{classification}")
    
    cleaned_content = '\n'.join(cleaned_parts)
    
    return cleaned_content


def format_qa_content(question: str, answer: str, source: str = "", add_type: str = "", classification: str = ""):
    """æ ¼å¼åŒ–é—®ç­”å¯¹å†…å®¹"""
    content = f"é—®:{question}\nç­”:{answer}"
    
    if source:
        content += f"\n#source#:{source}"
    
    if add_type:
        content += f"\næ·»åŠ äººå‘˜:{add_type}"
    
    if classification:
        content += f"\nåˆ†ç±»:{classification}"
    
    return content


def determine_add_method(document_id: str, add_type: str):
    """ç¡®å®šæ·»åŠ æ–¹å¼"""
    if document_id == "1a92b558-2051-4ebc-9441-2209dfd356b8":
        return "æ—§QA"
    elif document_id == "ee3a5cb0-3fa9-4cd1-9a1a-113bc43b5d5a":
        return "å¾®ä¿¡æ¯æ—¥QA"
    elif document_id == "a025564c-33b4-458e-835b-324ac75c0e24":
        # ä» add_type åˆ¤æ–­
        if add_type == "äººå·¥æ·»åŠ ":
            return "äººå·¥æ·»åŠ "
        elif add_type == "ç”¨æˆ·æ·»åŠ ":
            return "ç”¨æˆ·æ·»åŠ "
        else:
            return "æœªçŸ¥"
    else:
        return "æœªçŸ¥"


def determine_add_source(source: str):
    """ç¡®å®šæ·»åŠ æ¥æº"""
    return source if source else '-'


# ==================== è·¯ç”±æ¥å£ ====================

@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    return render_template('review_qa.html')


@app.route('/api/unreviewed/segments', methods=['GET'])
def get_unreviewed_segments():
    """è·å–æœªå®¡æ ¸åŒºåŸŸçš„æ‰€æœ‰åˆ†æ®µ"""
    try:
        # è°ƒç”¨æœ¬åœ°APIè·å–æ•°æ®
        dataset_id = UNREVIEWED_DATASET_ID
        api_url = f"{LOCAL_QUERY_API_BASE}?dataset_id={dataset_id}"
        
        logger.info(f"è¯·æ±‚æœ¬åœ°API: {api_url}")
        response = requests.get(api_url, timeout=30)
        
        if response.status_code != 200:
            logger.error(f"æœ¬åœ°APIè¯·æ±‚å¤±è´¥: status_code={response.status_code}")
            return jsonify({'success': False, 'error': f'æœ¬åœ°APIè¯·æ±‚å¤±è´¥: {response.status_code}'}), 500
        
        api_data = response.json()
        segments = api_data.get('data', [])
        
        if not segments:
            logger.warning("æœ¬åœ°APIè¿”å›æ•°æ®ä¸ºç©º")
            return jsonify({
                'success': True,
                'data': [],
                'total': 0
            })
        
        all_segments = []
        
        # éå†æ‰€æœ‰åˆ†æ®µï¼Œè¿›è¡Œæ•°æ®è½¬æ¢å’Œå¤„ç†
        for seg in segments:
            # 1. å­—æ®µåè½¬æ¢ï¼šsegment_id â†’ id
            if 'segment_id' in seg:
                seg['id'] = seg.pop('segment_id')
            elif 'id' not in seg:
                logger.warning(f"åˆ†æ®µç¼ºå°‘idå­—æ®µ: {seg}")
                continue
            
            # 2. æ—¶é—´æ ¼å¼è½¬æ¢ï¼šå­—ç¬¦ä¸²(UTC) â†’ æ—¶é—´æˆ³(ä¸œå…«åŒº)
            created_at_str = seg.get('created_at', '')
            if isinstance(created_at_str, str):
                try:
                    # è§£æä¸ºUTCæ—¶é—´
                    dt_utc = datetime.strptime(created_at_str, '%Y-%m-%d %H:%M:%S')
                    dt_utc = dt_utc.replace(tzinfo=timezone.utc)
                    # è½¬æ¢ä¸ºä¸œå…«åŒºæ—¶é—´ï¼ˆUTC+8ï¼‰
                    dt_cst = dt_utc.astimezone(timezone(timedelta(hours=8)))
                    seg['created_at'] = int(dt_cst.timestamp())
                except ValueError as e:
                    logger.warning(f"æ—¶é—´æ ¼å¼è§£æå¤±è´¥: {created_at_str}, é”™è¯¯: {e}")
                    seg['created_at'] = 0
            elif not isinstance(created_at_str, (int, float)):
                seg['created_at'] = 0
            
            # 3. å¤„ç† updated_atï¼šå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ created_at
            updated_at = seg.get('updated_at')
            if updated_at:
                if isinstance(updated_at, str):
                    try:
                        # è§£æä¸ºUTCæ—¶é—´
                        dt_utc = datetime.strptime(updated_at, '%Y-%m-%d %H:%M:%S')
                        dt_utc = dt_utc.replace(tzinfo=timezone.utc)
                        # è½¬æ¢ä¸ºä¸œå…«åŒºæ—¶é—´ï¼ˆUTC+8ï¼‰
                        dt_cst = dt_utc.astimezone(timezone(timedelta(hours=8)))
                        seg['updated_at'] = int(dt_cst.timestamp())
                    except ValueError:
                        seg['updated_at'] = seg.get('created_at', 0)
                elif not isinstance(updated_at, (int, float)):
                    seg['updated_at'] = seg.get('created_at', 0)
            else:
                seg['updated_at'] = seg.get('created_at', 0)
            
            # 4. è·å– document_nameï¼ˆæ ¹æ® document_id æŸ¥æ‰¾ï¼‰
            doc_id = seg.get('document_id', '')
            doc_name = UNREVIEWED_DOCUMENTS.get(doc_id, 'æœªçŸ¥æ–‡æ¡£')
            seg['document_name'] = doc_name
            
            # 5. ä¿ç•™åŸå§‹ content å­—æ®µï¼ˆç”¨äºåç»­è§£æï¼‰
            content = seg.get('content', '')
            
            # 5.5. æ¸…ç†contentï¼ˆè§„èŒƒåŒ–æ ¼å¼ï¼‰
            cleaned_content = clean_qa_content(content)
            
            # 6. è§£æQAå†…å®¹
            parsed = parse_qa_content(cleaned_content)
            
            seg['question'] = parsed.get('question', '')
            seg['answer'] = parsed.get('answer', '')
            seg['add_method'] = determine_add_method(doc_id, parsed.get('add_type', ''))
            seg['add_source'] = determine_add_source(parsed.get('source', ''))
            seg['classification'] = parsed.get('classification', '')
            
            all_segments.append(seg)
        
        # æŒ‰ updated_at é™åºæ’åˆ—ï¼ˆä¼˜å…ˆä½¿ç”¨ updated_atï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ created_atï¼‰
        all_segments.sort(key=lambda x: x.get('updated_at', x.get('created_at', 0)), reverse=True)
        
        logger.info(f"æˆåŠŸè·å– {len(all_segments)} ä¸ªæœªå®¡æ ¸åˆ†æ®µ")
        
        return jsonify({
            'success': True,
            'data': all_segments,
            'total': len(all_segments)
        })
        
    except requests.exceptions.RequestException as e:
        logger.error(f"æœ¬åœ°APIè¯·æ±‚å¼‚å¸¸: {e}")
        return jsonify({'success': False, 'error': f'æœ¬åœ°APIè¯·æ±‚å¼‚å¸¸: {str(e)}'}), 500
    except Exception as e:
        logger.error(f"è·å–æœªå®¡æ ¸åˆ†æ®µå¤±è´¥: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/reviewed/documents', methods=['GET'])
def get_reviewed_documents():
    """è·å–å·²å®¡æ ¸æ–‡æ¡£åˆ—è¡¨"""
    try:
        documents = [
            {'id': doc_id, 'name': doc_name}
            for doc_id, doc_name in REVIEWED_DOCUMENTS.items()
        ]
        
        return jsonify({
            'success': True,
            'data': documents
        })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/document-categories', methods=['GET'])
def get_document_categories():
    """è·å–æ‰€æœ‰æ–‡æ¡£åˆ†ç±»åˆ—è¡¨(ç”¨äºä¸‹æ‹‰é€‰æ‹©)"""
    try:
        categories = [
            {'id': doc_id, 'name': doc_name}
            for doc_id, doc_name in REVIEWED_DOCUMENTS.items()
        ]
        
        return jsonify({
            'success': True,
            'categories': categories,
            'total': len(categories)
        })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ†ç±»åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/reviewed/segments/<document_id>', methods=['GET'])
def get_reviewed_segments(document_id):
    """è·å–å·²å®¡æ ¸åŒºåŸŸæŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰åˆ†æ®µ"""
    try:
        if document_id not in REVIEWED_DOCUMENTS:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡æ¡£ID'}), 400
        
        client = DifyAPIClient()
        result = client.get_all_segments(REVIEWED_DATASET_ID, document_id)
        
        if not result['success']:
            return jsonify(result), 500
        
        segments = result['data']
        
        # ä¸ºæ¯ä¸ªåˆ†æ®µæ·»åŠ å…ƒæ•°æ®
        for segment in segments:
            content = segment.get('content', '')
            parsed = parse_qa_content(content)
            
            segment['document_id'] = document_id
            segment['document_name'] = REVIEWED_DOCUMENTS[document_id]
            segment['question'] = parsed['question']
            segment['answer'] = parsed['answer']
        
        # æŒ‰updated_até™åºæ’åº
        segments.sort(key=lambda x: x.get('updated_at', 0), reverse=True)
        
        return jsonify({
            'success': True,
            'data': segments,
            'total': len(segments)
        })
        
    except Exception as e:
        logger.error(f"è·å–å·²å®¡æ ¸åˆ†æ®µå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/segment/update', methods=['POST'])
def update_segment():
    """æ›´æ–°åˆ†æ®µå†…å®¹"""
    try:
        data = request.json
        dataset_id = data.get('dataset_id')
        document_id = data.get('document_id')
        segment_id = data.get('segment_id')
        question = data.get('question', '').strip()
        answer = data.get('answer', '').strip()
        
        if not all([dataset_id, document_id, segment_id, question, answer]):
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        # è·å–åŸåˆ†æ®µä»¥ä¿ç•™å…ƒæ•°æ®
        client = DifyAPIClient()
        result = client.get_document_segments(dataset_id, document_id)
        
        if not result['success']:
            return jsonify(result), 500
        
        # æŸ¥æ‰¾åŸåˆ†æ®µ
        original_segment = None
        for seg in result['data'].get('data', []):
            if seg['id'] == segment_id:
                original_segment = seg
                break
        
        if not original_segment:
            return jsonify({'success': False, 'error': 'åˆ†æ®µä¸å­˜åœ¨'}), 404
        
        # è§£æåŸå†…å®¹ä»¥ä¿ç•™å…ƒæ•°æ®
        original_content = original_segment.get('content', '')
        parsed = parse_qa_content(original_content)
        
        # æ„é€ æ–°å†…å®¹
        new_content = format_qa_content(
            question, 
            answer,
            parsed.get('source', ''),
            parsed.get('add_type', '')
        )
        
        # æ›´æ–°åˆ†æ®µ
        keywords = [question[:50]] if len(question) > 0 else []
        result = client.update_segment(dataset_id, document_id, segment_id, new_content, keywords)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"æ›´æ–°åˆ†æ®µå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/segment/delete', methods=['POST'])
def delete_segment():
    """åˆ é™¤åˆ†æ®µ"""
    try:
        data = request.json
        dataset_id = data.get('dataset_id')
        document_id = data.get('document_id')
        segment_id = data.get('segment_id')
        
        if not all([dataset_id, document_id, segment_id]):
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        client = DifyAPIClient()
        result = client.delete_segment(dataset_id, document_id, segment_id)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"åˆ é™¤åˆ†æ®µå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/reviewed/segment/<segment_id>', methods=['GET'])
def get_reviewed_segment_by_id(segment_id):
    """è·å–å•ä¸ªå·²å®¡æ ¸åˆ†æ®µ(RESTfulé£æ ¼)"""
    try:
        # éœ€è¦éå†æ‰€æœ‰æ–‡æ¡£æŸ¥æ‰¾è¯¥åˆ†æ®µ
        client = DifyAPIClient()
        
        for doc_id, doc_name in REVIEWED_DOCUMENTS.items():
            result = client.get_segment(REVIEWED_DATASET_ID, doc_id, segment_id)
            if result['success']:
                return jsonify(result)
        
        return jsonify({'success': False, 'error': 'åˆ†æ®µä¸å­˜åœ¨'}), 404
        
    except Exception as e:
        logger.error(f"è·å–åˆ†æ®µå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/reviewed/segments/<segment_id>', methods=['PUT'])
def update_reviewed_segment(segment_id):
    """æ›´æ–°å·²å®¡æ ¸åˆ†æ®µå†…å®¹(RESTfulé£æ ¼)"""
    try:
        data = request.json
        document_id = data.get('document_id')
        question = data.get('question', '').strip()
        answer = data.get('answer', '').strip()
        
        if not all([document_id, question, answer]):
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        # è·å–åŸåˆ†æ®µä»¥ä¿ç•™å…ƒæ•°æ®
        client = DifyAPIClient()
        result = client.get_segment(REVIEWED_DATASET_ID, document_id, segment_id)
        
        if not result['success']:
            return jsonify(result), 500
        
        original_segment = result['data']
        
        # è§£æåŸå†…å®¹ä»¥ä¿ç•™å…ƒæ•°æ®
        original_content = original_segment.get('content', '')
        parsed = parse_qa_content(original_content)
        
        # æ„é€ æ–°å†…å®¹
        new_content = format_qa_content(
            question, 
            answer,
            parsed.get('source', ''),
            parsed.get('add_type', ''),
            parsed.get('classification', '')
        )
        
        # æ›´æ–°åˆ†æ®µ
        keywords = [question[:50]] if len(question) > 0 else []
        result = client.update_segment(REVIEWED_DATASET_ID, document_id, segment_id, new_content, keywords)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"æ›´æ–°åˆ†æ®µå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/reviewed/segments/<segment_id>', methods=['DELETE'])
def delete_reviewed_segment(segment_id):
    """åˆ é™¤å·²å®¡æ ¸åˆ†æ®µ(RESTfulé£æ ¼)"""
    try:
        data = request.json
        document_id = data.get('document_id')
        
        if not document_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘document_idå‚æ•°'}), 400
        
        client = DifyAPIClient()
        result = client.delete_segment(REVIEWED_DATASET_ID, document_id, segment_id)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"åˆ é™¤åˆ†æ®µå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/segment/approve', methods=['POST'])
def approve_segment():
    """é€šè¿‡å®¡æ ¸ï¼ˆè½¬ç§»åˆ°å·²å®¡æ ¸çŸ¥è¯†åº“ï¼‰"""
    try:
        data = request.json
        source_document_id = data.get('source_document_id')
        segment_id = data.get('segment_id')
        target_document_id = data.get('target_document_id')
        question = data.get('question', '').strip()
        answer = data.get('answer', '').strip()
        
        if not all([source_document_id, segment_id, target_document_id, question, answer]):
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        if target_document_id not in REVIEWED_DOCUMENTS:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„ç›®æ ‡æ–‡æ¡£ID'}), 400
        
        client = DifyAPIClient()
        
        # 1. ä½¿ç”¨å•ä¸ªåˆ†æ®µæŸ¥è¯¢APIï¼ˆæœ€ä¼˜æ–¹æ¡ˆï¼‰
        result = client.get_segment(UNREVIEWED_DATASET_ID, source_document_id, segment_id)
        
        if not result['success']:
            logger.error(f"âŒ è·å–åˆ†æ®µå¤±è´¥: {segment_id}")
            return jsonify({'success': False, 'error': f"è·å–åŸåˆ†æ®µå¤±è´¥: {result.get('error')}"}), 500
        
        original_segment = result['data']
        
        if not original_segment:
            logger.error(f"âŒ åˆ†æ®µä¸å­˜åœ¨: {segment_id}")
            return jsonify({'success': False, 'error': 'åŸåˆ†æ®µä¸å­˜åœ¨'}), 404
        
        # 2. è§£æåŸå†…å®¹
        original_content = original_segment.get('content', '')
        parsed = parse_qa_content(original_content)
        
        # 3. æ„é€ æ–°å†…å®¹ï¼ˆä¿ç•™å…ƒæ•°æ®ï¼‰
        new_content = format_qa_content(
            question,
            answer,
            parsed.get('source', ''),
            parsed.get('add_type', '')
        )
        
        # 4. åœ¨ç›®æ ‡æ–‡æ¡£ä¸­æ·»åŠ åˆ†æ®µ
        keywords = [question[:50]] if len(question) > 0 else []
        add_result = client.add_segment(REVIEWED_DATASET_ID, target_document_id, new_content, keywords)
        
        if not add_result['success']:
            return jsonify({'success': False, 'error': f'æ·»åŠ åˆ°ç›®æ ‡æ–‡æ¡£å¤±è´¥: {add_result.get("error")}'}), 500
        
        # 5. åˆ é™¤åŸåˆ†æ®µ
        delete_result = client.delete_segment(UNREVIEWED_DATASET_ID, source_document_id, segment_id)
        
        if not delete_result['success']:
            logger.warning(f"âš ï¸ åˆ é™¤åŸåˆ†æ®µå¤±è´¥ï¼Œä½†å·²æ·»åŠ åˆ°ç›®æ ‡æ–‡æ¡£: {delete_result.get('error')}")
        
        target_doc_name = REVIEWED_DOCUMENTS.get(target_document_id, 'æœªçŸ¥æ–‡æ¡£')
        logger.info(f"âœ… å®¡æ ¸é€šè¿‡ [segment_id={segment_id}] -> [ç›®æ ‡æ–‡æ¡£={target_doc_name}]")
        
        # è®°å½•å®¡æ ¸ç»Ÿè®¡
        record_approval()
        
        return jsonify({
            'success': True,
            'message': f'å·²è½¬ç§»åˆ° {REVIEWED_DOCUMENTS[target_document_id]}'
        })
        
    except Exception as e:
        logger.error(f"å®¡æ ¸é€šè¿‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# å®¡æ ¸ç»Ÿè®¡æ•°æ®åº“ - ç»Ÿä¸€å­˜æ”¾åœ¨resource/dataæ–‡ä»¶å¤¹
# __file__: src/web_admin/review-QA/review_qa_backend.py
# parent: src/web_admin/review-QA/
# parent.parent: src/web_admin/
# parent.parent.parent: src/
# parent.parent.parent.parent: é¡¹ç›®æ ¹ç›®å½•
STATS_DB = Path(__file__).parent.parent.parent.parent / 'resource' / 'data' / 'approval_stats.db'

def init_stats_db():
    """åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®åº“"""
    import sqlite3
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    STATS_DB.parent.mkdir(parents=True, exist_ok=True)
    
    conn = sqlite3.connect(str(STATS_DB))
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS approval_stats (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            approval_date DATE NOT NULL,
            count INTEGER DEFAULT 0,
            UNIQUE(approval_date)
        )
    ''')
    
    conn.commit()
    conn.close()
    logger.info("âœ… å®¡æ ¸ç»Ÿè®¡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ [db_path=%s]", STATS_DB)

def record_approval():
    """è®°å½•ä¸€æ¬¡å®¡æ ¸"""
    import sqlite3
    from datetime import date
    
    today = date.today().isoformat()
    
    conn = sqlite3.connect(str(STATS_DB))
    cursor = conn.cursor()
    
    cursor.execute('''
        INSERT INTO approval_stats (approval_date, count)
        VALUES (?, 1)
        ON CONFLICT(approval_date) 
        DO UPDATE SET count = count + 1
    ''', (today,))
    
    conn.commit()
    conn.close()

@app.route('/api/stats/today', methods=['GET'])
def get_today_stats():
    """è·å–ä»Šæ—¥å®¡æ ¸ç»Ÿè®¡"""
    try:
        import sqlite3
        from datetime import date
        
        today = date.today().isoformat()
        
        conn = sqlite3.connect(str(STATS_DB))
        cursor = conn.cursor()
        
        cursor.execute('SELECT count FROM approval_stats WHERE approval_date = ?', (today,))
        result = cursor.fetchone()
        
        conn.close()
        
        count = result[0] if result else 0
        
        return jsonify({
            'success': True,
            'count': count,
            'date': today
        })
        
    except Exception as e:
        logger.error(f"è·å–ä»Šæ—¥ç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/stats/monthly', methods=['GET'])
def get_monthly_stats():
    """è·å–æœˆåº¦å®¡æ ¸ç»Ÿè®¡"""
    try:
        import sqlite3
        from datetime import date
        
        year = request.args.get('year', date.today().year, type=int)
        month = request.args.get('month', date.today().month, type=int)
        
        # æ„é€ æœˆä»½èŒƒå›´
        start_date = f"{year}-{month:02d}-01"
        if month == 12:
            end_date = f"{year + 1}-01-01"
        else:
            end_date = f"{year}-{month + 1:02d}-01"
        
        conn = sqlite3.connect(str(STATS_DB))
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT approval_date, count 
            FROM approval_stats 
            WHERE approval_date >= ? AND approval_date < ?
            ORDER BY approval_date
        ''', (start_date, end_date))
        
        results = cursor.fetchall()
        conn.close()
        
        # è½¬æ¢ä¸ºå­—å…¸
        stats = {row[0]: row[1] for row in results}
        
        return jsonify({
            'success': True,
            'stats': stats,
            'year': year,
            'month': month
        })
        
    except Exception as e:
        logger.error(f"è·å–æœˆåº¦ç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

# ç¼“å­˜å·²å®¡æ ¸æ€»æ•°
reviewed_total_cache = {'total': 0, 'timestamp': 0}
CACHE_DURATION = 300  # 5åˆ†é’Ÿç¼“å­˜

@app.route('/api/stats/total-reviewed', methods=['GET'])
def get_total_reviewed():
    """è·å–å·²å®¡æ ¸åŒºåŸŸæ€»æ¡æ•°ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        import time
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if current_time - reviewed_total_cache['timestamp'] < CACHE_DURATION:
            logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„å·²å®¡æ ¸æ€»æ•°: {reviewed_total_cache['total']}")
            return jsonify({
                'success': True,
                'total': reviewed_total_cache['total'],
                'cached': True
            })
        
        # ç¼“å­˜è¿‡æœŸï¼Œé‡æ–°è®¡ç®—
        client = DifyAPIClient()
        total = 0
        
        logger.info("ğŸ”„ é‡æ–°è®¡ç®—å·²å®¡æ ¸æ€»æ•°...")
        # éå†æ‰€æœ‰å·²å®¡æ ¸æ–‡æ¡£
        for doc_id in REVIEWED_DOCUMENTS.keys():
            result = client.get_all_segments(REVIEWED_DATASET_ID, doc_id)
            if result['success']:
                total += len(result['data'])
        
        # æ›´æ–°ç¼“å­˜
        reviewed_total_cache['total'] = total
        reviewed_total_cache['timestamp'] = current_time
        
        logger.info(f"âœ… å·²å®¡æ ¸æ€»æ•°: {total}")
        
        return jsonify({
            'success': True,
            'total': total,
            'cached': False
        })
        
    except Exception as e:
        logger.error(f"è·å–å·²å®¡æ ¸æ€»æ•°å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/reviewed/check-duplicates', methods=['POST'])
def check_duplicates():
    """æŸ¥é‡åŠŸèƒ½ - ä½¿ç”¨BGEæ¨¡å‹ + ä½™å¼¦ç›¸ä¼¼åº¦"""
    try:
        data = request.json
        similarity_threshold = data.get('similarity_threshold', 0.8)  # é»˜è®¤0.8 (80%)
        
        logger.info(f"ğŸ” å¼€å§‹æŸ¥é‡ [é˜ˆå€¼={similarity_threshold}]")
        
        # 1. åŠ è½½æ‰€æœ‰å·²å®¡æ ¸æ–‡æ¡£çš„åˆ†æ®µ
        client = DifyAPIClient()
        all_segments = []
        
        for doc_id, doc_name in REVIEWED_DOCUMENTS.items():
            result = client.get_all_segments(REVIEWED_DATASET_ID, doc_id)
            
            if result['success']:
                segments = result['data']
                
                # ä¸ºæ¯ä¸ªåˆ†æ®µæ·»åŠ å…ƒæ•°æ®
                for seg in segments:
                    content = seg.get('content', '')
                    parsed = parse_qa_content(content)
                    
                    seg['document_id'] = doc_id
                    seg['document_name'] = doc_name
                    seg['question'] = parsed['question']
                    seg['answer'] = parsed['answer']
                    seg['classification'] = parsed.get('classification', '-')
                    
                    all_segments.append(seg)
        
        logger.info(f"âœ… åŠ è½½å®Œæˆ [æ€»æ•°={len(all_segments)}]")
        
        # 2. è°ƒç”¨æŸ¥é‡å™¨
        checker = DuplicateChecker()
        duplicate_groups = checker.find_duplicates(
            all_segments, 
            similarity_threshold=similarity_threshold
        )
        
        # 3. æ ¼å¼åŒ–ç»“æœ
        result = checker.format_duplicate_groups(duplicate_groups)
        
        logger.info(f"âœ… æŸ¥é‡å®Œæˆ [é‡å¤ç»„={result['total_groups']}, é‡å¤æ¡ç›®={result['total_duplicates']}]")
        
        return jsonify({
            'success': True,
            'data': result
        })
        
    except Exception as e:
        logger.error(f"âŒ æŸ¥é‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500

if __name__ == '__main__':
    logger.info("="*60)
    logger.info("ğŸš€ QAå®¡æ ¸ä¸ä¿®æ­£ç³»ç»Ÿå¯åŠ¨")
    logger.info("="*60)
    logger.info(f"ğŸ“Š æœªå®¡æ ¸çŸ¥è¯†åº“ID: {UNREVIEWED_DATASET_ID}")
    logger.info(f"ğŸ“Š å·²å®¡æ ¸çŸ¥è¯†åº“ID: {REVIEWED_DATASET_ID}")
    logger.info(f"ğŸ“„ æœªå®¡æ ¸æ–‡æ¡£æ•°: {len(UNREVIEWED_DOCUMENTS)}")
    logger.info(f"ğŸ“„ å·²å®¡æ ¸æ–‡æ¡£æ•°: {len(REVIEWED_DOCUMENTS)}")
    logger.info("âœ¨ ä½¿ç”¨å•ä¸ªåˆ†æ®µæŸ¥è¯¢APIï¼Œæ•°æ®å®æ—¶åŒæ­¥")
    logger.info("="*60)
    
    # åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®åº“
    init_stats_db()
    
    logger.info("ğŸŒ æœåŠ¡å™¨å¯åŠ¨ä¸­... [http://0.0.0.0:5002]")
    app.run(host='0.0.0.0', port=5002, debug=True)
